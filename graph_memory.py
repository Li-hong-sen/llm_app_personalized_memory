
import os
import streamlit as st
from mem0 import Memory
from openai import OpenAI

# --- ä¿®å¤ mem0 ä¸Ž langchain_neo4j / Qwen çš„å…¼å®¹æ€§ bug ---
# Bug 1: langchain_neo4j æ–°ç‰ˆ Neo4jGraph.__init__ åœ¨ password å’Œ database ä¹‹é—´
#   æ–°å¢žäº† token å‚æ•°ï¼Œmem0 ç”¨ä½ç½®å‚æ•°ä¼ é€’å¯¼è‡´ database è¢«å½“ä½œ token -> AuthError
# Bug 2: Qwen æ¨¡åž‹ tool call è¿”å›žçš„å®žä½“å¯èƒ½ç¼ºå°‘ source/relationship/destination å­—æ®µ
#   å¯¼è‡´ _remove_spaces_from_entities æŠ›å‡º KeyError: 'source'
import logging
from mem0.memory.graph_memory import MemoryGraph, Neo4jGraph
from mem0.memory.utils import sanitize_relationship_for_cypher
from mem0.utils.factory import EmbedderFactory, LlmFactory

_original_init = MemoryGraph.__init__

def _patched_init(self, config):
    """ä¿®å¤ Neo4jGraph ä½ç½®å‚æ•° bugï¼Œæ”¹ç”¨å…³é”®å­—å‚æ•°"""
    self.config = config
    self.graph = Neo4jGraph(
        url=self.config.graph_store.config.url,
        username=self.config.graph_store.config.username,
        password=self.config.graph_store.config.password,
        database=self.config.graph_store.config.database,
        refresh_schema=False,
        driver_config={"notifications_min_severity": "OFF"},
    )
    self.embedding_model = EmbedderFactory.create(
        self.config.embedder.provider, self.config.embedder.config, self.config.vector_store.config
    )
    self.node_label = ":`__Entity__`" if self.config.graph_store.config.base_label else ""
    if self.config.graph_store.config.base_label:
        try:
            self.graph.query(f"CREATE INDEX entity_single IF NOT EXISTS FOR (n {self.node_label}) ON (n.user_id)")
        except Exception:
            pass
        try:
            self.graph.query(f"CREATE INDEX entity_composite IF NOT EXISTS FOR (n {self.node_label}) ON (n.name, n.user_id)")
        except Exception:
            pass
    self.llm_provider = "openai"
    if self.config.llm and self.config.llm.provider:
        self.llm_provider = self.config.llm.provider
    if self.config.graph_store and self.config.graph_store.llm and self.config.graph_store.llm.provider:
        self.llm_provider = self.config.graph_store.llm.provider
    llm_config = None
    if self.config.graph_store and self.config.graph_store.llm and hasattr(self.config.graph_store.llm, "config"):
        llm_config = self.config.graph_store.llm.config
    elif hasattr(self.config.llm, "config"):
        llm_config = self.config.llm.config
    self.llm = LlmFactory.create(self.llm_provider, llm_config)
    self.user_id = None
    self.threshold = self.config.graph_store.threshold if hasattr(self.config.graph_store, 'threshold') else 0.7

def _patched_remove_spaces(self, entity_list):
    """ä¿®å¤ Qwen tool call è¿”å›žå®žä½“å¯èƒ½ç¼ºå°‘å­—æ®µçš„é—®é¢˜ï¼Œè·³è¿‡ä¸å®Œæ•´çš„å®žä½“"""
    _logger = logging.getLogger(__name__)
    valid = []
    for item in entity_list:
        if not all(k in item for k in ("source", "relationship", "destination")):
            _logger.warning(f"è·³è¿‡ä¸å®Œæ•´çš„å®žä½“: {item}")
            continue
        item["source"] = item["source"].lower().replace(" ", "_")
        item["relationship"] = sanitize_relationship_for_cypher(item["relationship"].lower().replace(" ", "_"))
        item["destination"] = item["destination"].lower().replace(" ", "_")
        valid.append(item)
    return valid

MemoryGraph.__init__ = _patched_init
MemoryGraph._remove_spaces_from_entities = _patched_remove_spaces
# --- ä¿®å¤ç»“æŸ ---

st.title("LLM App with Memory ðŸ§ ")
st.caption("LLM App with personalized memory layer that remembers ever user's choice and interests")

# é˜¿é‡Œäº‘ DashScope API Key
dashscope_api_key = st.text_input("Enter Aliyun DashScope API Key", value="sk-37b98fd30e1d44669bb1046aac45dc69", type="password")

@st.cache_resource
def init_client(api_key):
    """ç¼“å­˜ OpenAI å®¢æˆ·ç«¯"""
    return OpenAI(
        api_key=api_key,
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
    )

@st.cache_resource
def init_memory(api_key):
    """ç¼“å­˜ Memory å¯¹è±¡"""
    config = {
        "graph_store": {
        "provider": "neo4j",
        "config": {
            "url": "neo4j+s://7ebeea71.databases.neo4j.io",
            "username": "neo4j",
            "password": "jkHfTyOWwicagj1Dc18Vw_-pmJdZ2IdMKUXYFKYYuts",
            "database": "neo4j",
        }
    },
        "llm": {
            "provider": "openai",
            "config": {
                "model": "qwen-plus",
                "api_key": api_key,
                "openai_base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1"
            }
        },
        "embedder": {
            "provider": "openai",
            "config": {
                "model": "text-embedding-v3",
                "api_key": api_key,
                "openai_base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
                "embedding_dims": 1024
            }
        }
    }
    return Memory.from_config(config)

if dashscope_api_key:
    try:
        with st.spinner("æ­£åœ¨åˆå§‹åŒ–..."):
            client = init_client(dashscope_api_key)
            memory = init_memory(dashscope_api_key)
        st.success("åˆå§‹åŒ–å®Œæˆ!")
    except Exception as e:
        st.error(f"åˆå§‹åŒ–å¤±è´¥: {e}")
        st.stop()

    user_id = st.text_input("Enter your Username")

    prompt = st.text_input("Ask Qwen")

    if st.button('Chat with LLM'):
        if not user_id:
            st.warning("è¯·å…ˆè¾“å…¥ç”¨æˆ·å")
            st.stop()
        if not prompt:
            st.warning("è¯·è¾“å…¥é—®é¢˜")
            st.stop()
            
        try:
            with st.spinner('æ­£åœ¨æœç´¢è®°å¿†...'):
                relevant_memories = memory.search(query=prompt, user_id=user_id,    limit=3,rerank=True)
            st.success("è®°å¿†æœç´¢å®Œæˆ")
            
            # Prepare context with relevant memories
            context = "Relevant past information:\n"
            if relevant_memories:
                # å‘é‡å­˜å‚¨è®°å¿†
                for mem in relevant_memories.get("results", []):
                    memory_text = mem.get('memory', '') or mem.get('text', '')
                    if memory_text:
                        context += f"- {memory_text}\n"
                # å›¾è°±å­˜å‚¨è®°å¿† (å…³ç³»ä¸‰å…ƒç»„)
                for rel in relevant_memories.get("relations", []):
                    source = rel.get("source", "")
                    relationship = rel.get("relationship", "")
                    target = rel.get("target", rel.get("destination", ""))
                    if source and relationship and target:
                        context += f"- {source} {relationship} {target}\n"
                
            # Prepare the full prompt
            full_prompt = f"{context}\nHuman: {prompt}\nAI:"

            with st.spinner('æ­£åœ¨è°ƒç”¨ Qwen æ¨¡åž‹...'):
                # Get response from Qwen
                response = client.chat.completions.create(
                    model="qwen-plus",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant with access to past conversations."},
                        {"role": "user", "content": full_prompt}
                    ]
                )
            
            answer = response.choices[0].message.content
            st.write("Answer: ", answer)

            # Add conversation (user + AI) to memory
            with st.spinner('æ­£åœ¨ä¿å­˜åˆ°è®°å¿†...'):
                messages_to_save = [
                    {"role": "user", "content": prompt},
                    {"role": "assistant", "content": answer},
                ]
                add_result = memory.add(messages_to_save, user_id=user_id)
            st.success("å·²ä¿å­˜åˆ°è®°å¿†")
            # è°ƒè¯•: æ˜¾ç¤º add è¿”å›žç»“æžœ
            with st.expander("è°ƒè¯•: memory.add è¿”å›žå€¼"):
                st.json(add_result)
            
        except Exception as e:
            st.error(f"å‡ºé”™äº†: {e}")
            import traceback
            st.code(traceback.format_exc())


    # Sidebar option to show memory
    st.sidebar.title("Memory Info")
    if st.button("View My Memory"):
            memories = memory.get_all(user_id=user_id)

            # è°ƒè¯•: æ˜¾ç¤º get_all åŽŸå§‹è¿”å›ž
            with st.expander("è°ƒè¯•: memory.get_all è¿”å›žå€¼"):
                st.json(memories)

            has_data = False

            # æ˜¾ç¤ºå‘é‡å­˜å‚¨çš„è®°å¿† (results)
            if memories.get("results"):
                st.write(f"**{user_id} çš„æ–‡æœ¬è®°å¿†:**")
                for mem in memories["results"]:
                    memory_text = mem.get('memory', '') or mem.get('text', '')
                    if memory_text:
                        st.write(f"- {memory_text}")
                        has_data = True

            # æ˜¾ç¤ºå›¾è°±å­˜å‚¨çš„è®°å¿† (relations)
            if memories.get("relations"):
                st.write(f"**{user_id} çš„å›¾è°±è®°å¿† (çŸ¥è¯†å›¾è°±):**")
                for rel in memories["relations"]:
                    source = rel.get("source", "?")
                    relationship = rel.get("relationship", "?")
                    target = rel.get("target", rel.get("destination", "?"))
                    st.write(f"- {source} --[{relationship}]--> {target}")
                    has_data = True

            if not has_data:
                st.info("No memory found for this user ID.")